{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   store_nbr      family  sales  onpromotion  dcoilwtico   type_x type_y  \\\n",
      "0          1  AUTOMOTIVE    0.0            0         NaN  Holiday      D   \n",
      "1          1   BABY CARE    0.0            0         NaN  Holiday      D   \n",
      "2          1      BEAUTY    0.0            0         NaN  Holiday      D   \n",
      "3          1   BEVERAGES    0.0            0         NaN  Holiday      D   \n",
      "4          1       BOOKS    0.0            0         NaN  Holiday      D   \n",
      "\n",
      "   cluster  \n",
      "0       13  \n",
      "1       13  \n",
      "2       13  \n",
      "3       13  \n",
      "4       13  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "AMOUNT_OF_DATA = 160000\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "    \"data/final.csv\"\n",
    ")[:AMOUNT_OF_DATA].drop(columns=[\"id\"]) # ID won't be usefull during training, so just dropping it.\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       store_nbr  onpromotion  dcoilwtico  cluster  family_BABY CARE  \\\n",
      "70164         28            0    94.32576       10             False   \n",
      "15784         51            0    93.08000       17             False   \n",
      "67672          8            0    95.84000        8             False   \n",
      "97134         34            0    94.32576        6             False   \n",
      "26570         54            0    93.26000        3             False   \n",
      "\n",
      "       family_BEAUTY  family_BEVERAGES  family_BOOKS  family_BREAD/BAKERY  \\\n",
      "70164          False             False         False                False   \n",
      "15784          False             False         False                False   \n",
      "67672          False             False         False                False   \n",
      "97134          False             False         False                False   \n",
      "26570          False             False         False                 True   \n",
      "\n",
      "       family_CELEBRATION  ...  family_POULTRY  family_PREPARED FOODS  \\\n",
      "70164                True  ...           False                  False   \n",
      "15784               False  ...           False                  False   \n",
      "67672               False  ...           False                  False   \n",
      "97134               False  ...           False                  False   \n",
      "26570               False  ...           False                  False   \n",
      "\n",
      "       family_PRODUCE  family_SCHOOL AND OFFICE SUPPLIES  family_SEAFOOD  \\\n",
      "70164           False                              False           False   \n",
      "15784           False                              False           False   \n",
      "67672           False                              False           False   \n",
      "97134           False                              False           False   \n",
      "26570           False                              False           False   \n",
      "\n",
      "       type_x_Work Day  type_y_B  type_y_C  type_y_D  type_y_E  \n",
      "70164            False     False     False     False      True  \n",
      "15784            False     False     False     False     False  \n",
      "67672            False     False     False      True     False  \n",
      "97134            False      True     False     False     False  \n",
      "26570            False     False      True     False     False  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "        store_nbr  onpromotion  dcoilwtico  cluster  family_BABY CARE  \\\n",
      "26172          43            0    93.26000       10             False   \n",
      "152087         26            0    96.53000       10             False   \n",
      "35245          48            0    94.32576       14              True   \n",
      "72315          38            0    94.32576        4             False   \n",
      "82906          35            0    94.32576        3             False   \n",
      "\n",
      "        family_BEAUTY  family_BEVERAGES  family_BOOKS  family_BREAD/BAKERY  \\\n",
      "26172           False              True         False                False   \n",
      "152087          False             False         False                False   \n",
      "35245           False             False         False                False   \n",
      "72315           False             False         False                False   \n",
      "82906           False             False         False                False   \n",
      "\n",
      "        family_CELEBRATION  ...  family_POULTRY  family_PREPARED FOODS  \\\n",
      "26172                False  ...           False                  False   \n",
      "152087               False  ...           False                  False   \n",
      "35245                False  ...           False                  False   \n",
      "72315                False  ...           False                  False   \n",
      "82906                False  ...           False                  False   \n",
      "\n",
      "        family_PRODUCE  family_SCHOOL AND OFFICE SUPPLIES  family_SEAFOOD  \\\n",
      "26172            False                              False           False   \n",
      "152087           False                              False           False   \n",
      "35245            False                              False           False   \n",
      "72315            False                              False           False   \n",
      "82906            False                              False           False   \n",
      "\n",
      "        type_x_Work Day  type_y_B  type_y_C  type_y_D  type_y_E  \n",
      "26172             False     False     False     False      True  \n",
      "152087            False     False     False      True     False  \n",
      "35245             False     False     False     False     False  \n",
      "72315             False     False     False      True     False  \n",
      "82906             False     False      True     False     False  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Seperating the dataset into X, Y and then splitting that into a test and a train set \n",
    "X = dataset.drop('sales', axis=1)\n",
    "y = dataset['sales']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# The oil dataset is not completely filled, so we fill it in with the mean oil price\n",
    "X_train['dcoilwtico'] = X_train['dcoilwtico'].fillna(X_train['dcoilwtico'].mean())\n",
    "X_test['dcoilwtico'] = X_test['dcoilwtico'].fillna(X_train['dcoilwtico'].mean())\n",
    "\n",
    "X_train = pd.get_dummies(X_train, columns=['family', 'type_x', 'type_y'], drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=['family', 'type_x', 'type_y'], drop_first=True)\n",
    "\n",
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem with the dataset is that some of the numbers are not between 0 and 1, which the program expects, welp... that's where the standardScaler comes in. It very simply just makes the numbers between them. We could write our own code for this, but the internet told me to do this, so I will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['dcoilwtico', 'onpromotion', 'store_nbr', 'cluster']\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearRegression\n",
    "\n",
    "Very simple linear model. I am not sure how it takes the class labels into account, but I assume it just sees each possible label as it's own axis (so in that case we would have 45 axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error:  184462.67820427864\n",
      "R2 score 0.5790034196055058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Mean squared error: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"R2 score\", r2_score(y_test, y_pred))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual sales\")\n",
    "plt.ylabel(\"predicted Sales\")\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.plot([0, max(y_test)], [0, max(y_test)], \"r--\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neighbour\n",
    "\n",
    "So, since sklearn is actually really easy to use, this wasn't hard to implement. Just for fun I made a forloop that goes trough a few of the K values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_squared_error: 66777.09608786298\n",
      "R2 score: 0.8475955712269779\n",
      "Mean_squared_error: 49091.92859243429\n",
      "R2 score: 0.887958180681418\n",
      "Mean_squared_error: 48643.18065211797\n",
      "R2 score: 0.8889823518046575\n",
      "Mean_squared_error: 48238.035379106805\n",
      "R2 score: 0.8899070091725387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "testList = [1, 3, 5, 7]\n",
    "\n",
    "for i in testList:\n",
    "    model = KNeighborsRegressor(n_neighbors=i)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predicted_values = model.predict(X_test)\n",
    "\n",
    "    print(\"Mean_squared_error:\", mean_squared_error(y_test, predicted_values))\n",
    "    print(\"R2 score:\", r2_score(y_test, predicted_values))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(y_test, predicted_values)\n",
    "    plt.xlabel(\"Actual sales\")\n",
    "    plt.ylabel(\"Predicted sales\")\n",
    "    plt.title(f\"KNN with {i} neighbors\")\n",
    "    plt.plot([0, max(y_test)], [0, max(y_test)], \"r--\") # guideline thingy\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "A similar implementation to the ones above, just using a different model to see which ones suit best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error:  36291.506371652526\n",
      "R2 score:  0.9171724045830517\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth=64, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "predicted_values = model.predict(X_test)\n",
    "\n",
    "print(\"Mean squared error: \", mean_squared_error(y_test, predicted_values))\n",
    "print(\"R2 score: \", r2_score(y_test, predicted_values))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_test, predicted_values)\n",
    "plt.xlabel(\"Actual Sales\")\n",
    "plt.ylabel(\"Predicted Sales\")\n",
    "plt.title(f\"Decision Tree (depth of 64)\")\n",
    "plt.plot([0, max(y_test)], [0, max(y_test)], \"r--\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error:  33638.4962778328\n",
      "R2 score:  0.9232273322688207\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=64, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "predicted_values = model.predict(X_test)\n",
    "\n",
    "print(\"Mean squared error: \", mean_squared_error(y_test, predicted_values))\n",
    "print(\"R2 score: \", r2_score(y_test, predicted_values))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_test, predicted_values)\n",
    "plt.xlabel(\"Actual Sales\")\n",
    "plt.ylabel(\"Predicted Sales\")\n",
    "plt.title(\"Random Forest, 64 estimators\")\n",
    "plt.plot([0, max(y_test)], [0, max(y_test)], \"r--\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kruskal-Wallis test results for MSE:\n",
      "H-statistic: 3.9381818181818176, p-value: 0.04720176769014221\n",
      "Kruskal-Wallis test results for R²:\n",
      "H-statistic: 2.8097560975609794, p-value: 0.09369261949324835\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "At least 3 sets of samples must be given for Friedman test, got 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [69], line 83\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH-statistic: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2_results\u001b[38;5;241m.\u001b[39mstatistic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, p-value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2_results\u001b[38;5;241m.\u001b[39mpvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# mse_flat = [value for model in mse_values for value in model]\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# r2_flat = [value for model in r2_values for value in model]\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# shapiro_r2_stat, shapiro_r2_p = stats.shapiro(r2_flat)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# print(f'Shapiro-Wilk Test for R-squared: Statistic={shapiro_r2_stat}, p-value={shapiro_r2_p}')\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m friedman_mse_results \u001b[38;5;241m=\u001b[39m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfriedmanchisquare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmse_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFriedman test results for MSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH-statistic: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfriedman_mse_results\u001b[38;5;241m.\u001b[39mstatistic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, p-value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfriedman_mse_results\u001b[38;5;241m.\u001b[39mpvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adast\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:586\u001b[0m, in \u001b[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    583\u001b[0m     res \u001b[38;5;241m=\u001b[39m _add_reduced_axes(res, reduced_axes, keepdims)\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tuple_to_result(\u001b[38;5;241m*\u001b[39mres)\n\u001b[1;32m--> 586\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mhypotest_fun_out\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m res \u001b[38;5;241m=\u001b[39m result_to_tuple(res, n_out)\n\u001b[0;32m    588\u001b[0m res \u001b[38;5;241m=\u001b[39m _add_reduced_axes(res, reduced_axes, keepdims)\n",
      "File \u001b[1;32mc:\\Users\\adast\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:8689\u001b[0m, in \u001b[0;36mfriedmanchisquare\u001b[1;34m(*samples)\u001b[0m\n\u001b[0;32m   8687\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(samples)\n\u001b[0;32m   8688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m-> 8689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAt least 3 sets of samples must be given \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   8690\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor Friedman test, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   8692\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(samples[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   8693\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, k):\n",
      "\u001b[1;31mValueError\u001b[0m: At least 3 sets of samples must be given for Friedman test, got 2."
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plts\n",
    "\n",
    "# MSE values for each model\n",
    "\n",
    "mse_values = [\n",
    "    # [178768.191, 185638.113, 162564.932, 201644.248, 189083.906],  # Linear Regression\n",
    "    # [72236.518, 49957.147, 66318.39, 60139.697, 56238.727],   # KNN (k = 1)\n",
    "    # [46854.559, 43213.887, 45083.834, 52086.285, 47649.818],   # KNN (k = 3)\n",
    "    # [40370.251, 42420.620, 42557.732, 50782.059, 48704.903],   # KNN (k = 5)\n",
    "    # [37602.404, 42221.357, 43185.170, 48782.908, 47929.983]   # KNN (k = 7)\n",
    "    [31466.628, 28553.271, 36734.049, 35063.619, 36583.474],   # Random Forests\n",
    "    [41658.577, 32363.860, 52799.208, 39940.704, 39306.239]    # Decision Trees\n",
    "]\n",
    "# avg = 0\n",
    "# for model in mse_values:\n",
    "#     for value in model:\n",
    "#         avg += value\n",
    "#     print(f\"Average MSE: {avg/5}\\n\")\n",
    "#     avg = 0\n",
    "\n",
    "mse_results = stats.kruskal(*mse_values)\n",
    "print(\"Kruskal-Wallis test results for MSE:\")\n",
    "print(f\"H-statistic: {mse_results.statistic}, p-value: {mse_results.pvalue}\")\n",
    "\n",
    "# R-squared values for each model\n",
    "r2_values = [\n",
    "    # [0.581, 0.564, 0.582, 0.564, 0.557],  # Linear Regression\n",
    "    # [0.830, 0.882, 0.829, 0.870, 0.868],  # KNN (k = 1)\n",
    "    # [0.890, 0.898, 0.884, 0.887, 0.888],  # KNN (k = 3)\n",
    "    # [0.905, 0.900, 0.890, 0.890, 0.886],  # KNN (k = 5)\n",
    "    # [0.911, 0.900, 0.888, 0.894, 0.887]  # KNN (k = 7)\n",
    "    [0.926, 0.933, 0.905, 0.924, 0.914],  # Random Forests\n",
    "    [0.902, 0.924, 0.864, 0.913, 0.908]   # Decision Trees\n",
    "]\n",
    "\n",
    "# avg = 0\n",
    "# for model in r2_values:\n",
    "#     for value in model:\n",
    "#         avg += value\n",
    "#     print(f\"Average r2: {avg/5}\")\n",
    "#     avg = 0\n",
    "\n",
    "# Perform ANOVA\n",
    "r2_results = stats.kruskal(*r2_values)\n",
    "print(\"Kruskal-Wallis test results for R²:\")\n",
    "print(f\"H-statistic: {r2_results.statistic}, p-value: {r2_results.pvalue}\")\n",
    "\n",
    "# mse_flat = [value for model in mse_values for value in model]\n",
    "# r2_flat = [value for model in r2_values for value in model]\n",
    "\n",
    "\n",
    "# def plot_distribution(data, title):\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "\n",
    "#     # Histogram\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.hist(data, bins=10, alpha=0.7, color='blue')\n",
    "#     plt.title(f'Histogram of {title}')\n",
    "#     plt.xlabel(title)\n",
    "#     plt.ylabel('Frequency')\n",
    "\n",
    "#     # Q-Q Plot\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "#     plt.title(f'Q-Q Plot of {title}')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "# # Plot distributions\n",
    "# plot_distribution(mse_flat, 'MSE Values')\n",
    "# plot_distribution(r2_flat, 'R-squared Values')\n",
    "\n",
    "# # Shapiro-Wilk Test for MSE\n",
    "# shapiro_mse_stat, shapiro_mse_p = stats.shapiro(mse_flat)\n",
    "# print(f'Shapiro-Wilk Test for MSE: Statistic={shapiro_mse_stat}, p-value={shapiro_mse_p}')\n",
    "\n",
    "# # Shapiro-Wilk Test for R-squared\n",
    "# shapiro_r2_stat, shapiro_r2_p = stats.shapiro(r2_flat)\n",
    "# print(f'Shapiro-Wilk Test for R-squared: Statistic={shapiro_r2_stat}, p-value={shapiro_r2_p}')\n",
    "\n",
    "friedman_mse_results = stats.friedmanchisquare(*mse_values)\n",
    "print(\"Friedman test results for MSE:\")\n",
    "print(f\"H-statistic: {friedman_mse_results.statistic}, p-value: {friedman_mse_results.pvalue}\")\n",
    "\n",
    "# Perform Friedman test for R²\n",
    "friedman_r2_results = stats.friedmanchisquare(*r2_values)\n",
    "print(\"Friedman test results for R²:\")\n",
    "print(f\"H-statistic: {friedman_r2_results.statistic}, p-value: {friedman_r2_results.pvalue}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "dimensions = X_train.shape[1]\n",
    "\n",
    "print(dimensions)\n",
    "\n",
    "model_nn = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=dimensions),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_nn.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model_nn.fit(X_train, y_train, validation_split=0.2, epochs=50, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "nn_pred = model_nn.predict(X_test)\n",
    "\n",
    "print(\"Neural Network MSE: \", mean_squared_error(y_test, nn_pred))\n",
    "print(\"Neural Network R2: \", r2_score(y_test, nn_pred))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_test, nn_pred)\n",
    "plt.xlabel(\"Actual Sales\")\n",
    "plt.ylabel(\"Predicted Sales\")\n",
    "plt.title(\"Neural Network Predictions\")\n",
    "plt.plot([0, max(y_test)], [0, max(y_test)], \"r--\")\n",
    "plt.show()\n",
    "\n",
    "plt.gcf().savefig(\"neural_network_predictions.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
